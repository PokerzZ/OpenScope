"""OpenDigger data collection utilities for OpenScope."""

import os
import subprocess
import pandas as pd
import json
import re
import stat
import logging
from typing import Optional, Sequence

# --- 1. åŠ¨æ€ç¯å¢ƒé…ç½®ï¼šæ”¯æŒå­æ–‡ä»¶å¤¹ ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# æŒ‡å®šå­æ–‡ä»¶å¤¹è·¯å¾„å’ŒäºŒè¿›åˆ¶æ–‡ä»¶å
SUB_DIR_NAME = "opendigger-cli"
BINARY_NAME = "od-cli"
DEFAULT_TIMEOUT_SECONDS = 60
DEFAULT_METRICS = [
    "openrank",
    "activity",
    "issue_response_time",
    "change_request_response_time",
    "inactive_contributors",
]
SAFE_REPO_SEPARATOR = "_"

# è®¡ç®—äºŒè¿›åˆ¶æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
BIN_PATH = os.path.join(BASE_DIR, SUB_DIR_NAME, BINARY_NAME)
# è®¡ç®—å­æ–‡ä»¶å¤¹çš„ç»å¯¹è·¯å¾„ï¼Œç”¨äºæ³¨å…¥ PATH
BIN_DIR_PATH = os.path.join(BASE_DIR, SUB_DIR_NAME)

# åŠ¨æ€å°†â€œäºŒè¿›åˆ¶æ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹â€åŠ å…¥å½“å‰è¿›ç¨‹çš„ PATH
os.environ["PATH"] = BIN_DIR_PATH + os.pathsep + os.environ["PATH"]

class OpenPuppeteerDataCore:
    """Core utilities for downloading and assembling OpenDigger metrics."""
    def __init__(self, binary_name: str = BINARY_NAME):
        self.binary_name = binary_name
        self.storage_dir = os.path.join(BASE_DIR, "data_warehouse")
        
        self._health_check()
        
        if not os.path.exists(self.storage_dir):
            os.makedirs(self.storage_dir, exist_ok=True)
            logging.info("Created OpenDigger storage directory: %s", self.storage_dir)

    def _health_check(self):
        """æ£€æŸ¥å­æ–‡ä»¶å¤¹å†…çš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å¯æ‰§è¡Œ"""
        if not os.path.exists(BIN_PATH):
            raise FileNotFoundError(f"âŒ é”™è¯¯ï¼šåœ¨ {BIN_PATH} æ‰¾ä¸åˆ°äºŒè¿›åˆ¶æ–‡ä»¶ï¼")
        
        # è‡ªåŠ¨ä¿®å¤æ‰§è¡Œæƒé™
        st = os.stat(BIN_PATH)
        if not (st.st_mode & stat.S_IEXEC):
            logging.info(f"ğŸ”§ è‡ªåŠ¨ä¿®å¤å­æ–‡ä»¶å¤¹å†… {self.binary_name} çš„æ‰§è¡Œæƒé™...")
            os.chmod(BIN_PATH, st.st_mode | stat.S_IEXEC)

    def fetch_and_clean(self, repo: str, metric: str) -> Optional[pd.DataFrame]:
        """Download and normalize a single OpenDigger metric."""
        safe_repo = repo.replace("/", SAFE_REPO_SEPARATOR)
        file_path = os.path.join(self.storage_dir, f"{safe_repo}_{metric}.json")
        
        # å› ä¸ºæˆ‘ä»¬å·²ç»æŠŠå­æ–‡ä»¶å¤¹åŠ å…¥äº† PATHï¼Œæ‰€ä»¥è¿™é‡Œç›´æ¥å†™åå­—å³å¯
        cmd = [self.binary_name, "download", repo, metric, "-o", file_path]
        
        logging.info(
            "Downloading OpenDigger metric '%s' for %s -> %s",
            metric,
            repo,
            file_path,
        )
        try:
            # check=True ä¼šåœ¨å‘½ä»¤å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
            subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                check=True,
                timeout=DEFAULT_TIMEOUT_SECONDS,
            )
            with open(file_path, 'r') as f:
                raw = json.load(f)
            
            monthly = {k: v for k, v in raw.items() if re.match(r'^\d{4}-\d{2}$', k)}
            df = pd.DataFrame(list(monthly.items()), columns=['month', metric])
            df['month'] = pd.to_datetime(df['month'])
            return df
        except subprocess.CalledProcessError as e:
            logging.error(f"âŒ {repo} {metric} æŠ“å–å¤±è´¥ï¼å‘½ä»¤è¡Œè¾“å‡º: {e.stderr}")
            return None
        except subprocess.TimeoutExpired:
            logging.error(
                f"âŒ {repo} {metric} æŠ“å–è¶…æ—¶ï¼ˆ{DEFAULT_TIMEOUT_SECONDS}sï¼‰ã€‚"
            )
            return None

    def build_aligned_dataset(
        self, repo: str, metrics: Optional[Sequence[str]] = None
    ) -> Optional[pd.DataFrame]:
        """Merge OpenDigger metrics into a single aligned dataset."""
        if metrics is None:
            # é»˜è®¤æŒ‡æ ‡é›†ï¼ŒåŒ…å«æ ¸å¿ƒæ´»è·ƒåº¦ã€å“åº”é€Ÿåº¦å’Œè´¡çŒ®è€…æµå¤±æƒ…å†µ
            metrics = DEFAULT_METRICS
        
        dfs = []
        for metric in metrics:
            df = self.fetch_and_clean(repo, metric)
            if df is not None:
                dfs.append(df)
        
        if not dfs:
            return None
            
        # æŒ‰ 'month' åˆ—åˆå¹¶æ‰€æœ‰æ•°æ®æ¡†
        final_df = dfs[0]
        for df in dfs[1:]:
            final_df = pd.merge(final_df, df, on='month', how='outer')
            
        final_df = final_df.sort_values('month')
        pd.set_option('future.no_silent_downcasting', True)
        final_df = final_df.infer_objects(copy=False).fillna(0)
        
        # --- ç‰¹å¾å·¥ç¨‹ ---
        if 'openrank' in final_df.columns:
            final_df['rank_velocity'] = final_df['openrank'].diff().fillna(0)
            
        # --- æ ‡ç­¾ç”Ÿæˆ (æµå¤±é£é™©) ---
        # è§„åˆ™ 1: æ´»è·ƒåº¦éª¤é™ (Activity Churn Risk)
        # å®šä¹‰: å½“æœˆæ´»è·ƒåº¦ä½äºè¿‡å» 3 ä¸ªæœˆå¹³å‡å€¼çš„ 50%
        if 'activity' in final_df.columns:
            final_df['activity_ma3'] = final_df['activity'].rolling(window=3).mean().fillna(0)
            final_df['churn_risk_activity'] = final_df.apply(
                lambda row: 1 if row['activity_ma3'] > 0 and row['activity'] < 0.5 * row['activity_ma3'] else 0, 
                axis=1
            )

        # è§„åˆ™ 2: è´¡çŒ®è€…æµå¤± (Contributor Churn Signal)
        # å®šä¹‰: éæ´»è·ƒè´¡çŒ®è€…æ•°é‡æ˜¾è‘—å¢åŠ  (ä¾‹å¦‚è¶…è¿‡ä¸Šä¸ªæœˆçš„ 20%)
        if 'inactive_contributors' in final_df.columns:
             final_df['inactive_diff'] = final_df['inactive_contributors'].diff().fillna(0)
             final_df['churn_risk_contributor'] = final_df.apply(
                 lambda row: 1 if row['inactive_diff'] > 5 else 0, # é˜ˆå€¼å¯è°ƒ
                 axis=1
             )

        return final_df

if __name__ == "__main__":
    core = OpenPuppeteerDataCore()
    data = core.build_aligned_dataset("X-lab2017/open-digger")
    
    if data is not None:
        print("\nâœ… [OpenPuppeteer-Rank] æ•°æ®é›†æˆæˆåŠŸï¼")
        print(data.tail(5))
